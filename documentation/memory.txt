mempool.cc interface:

- pages:
void* alloc_page()
{
    void *p = untracked_alloc_page();
    tracker_remember(p, page_size);
    return p;
}

void free_page(void* v)
{
    untracked_free_page(v);
    tracker_forget(v);
}

- huge pages:
void* alloc_huge_page(size_t N)
void free_huge_page(void* v, size_t N)

- boot
void free_initial_memory_range(void* addr, size_t size)

- standard
void* malloc(size_t size);
void* calloc(size_t nmemb, size_t size)
void* realloc(void* obj, size_t size)

int posix_memalign(void **memptr, size_t alignment, size_t size)
void *aligned_alloc(size_t alignment, size_t size)
void *memalign(size_t alignment, size_t size)
{
    return aligned_alloc(alignment, size);
}

void free(void* object);
size_t malloc_usable_size(void *object);

***************************************************
memory::pool::max_object_size       - 1024
memory::pool::min_object_size       - 8
sizeof(memory::pool::page_header)   - 40

    struct page_header {
        pool* owner;
        unsigned cpu_id;
        unsigned nalloc; //number of objects allocated
        bi::list_member_hook<> free_link;
        free_object* local_free;  // free objects in this page -> if null means that page is full
    };

0x0000100000000000 - memory area size

0xffff800000000000 - main memory area base
0xffff900000000000 - page memory area base
0xffffa00000000000 - mempool memory area base
0xffffb00000000000 - debug memory area base

******************************************
* 3 ways to allocate memory (from std_malloc) - in each case calls mmu::translate_mem_area() 
  to translate from main memory area to main or mempool one (for large keeps main)
-------------------------------------------------------
- malloc_pool::alloc() -    size <= 1024 (1/4 of page)
-------------------------------------------------------
calls memory::malloc_pools[n].alloc()
 - which adds a page (4K) if none to given size-determined (log2) malloc pool
    - pool::add_page():
        - creates page header and builds linked-list of free objects
        - adds new page to _free - list of free pages
 - and finds page header and next available free object of size matching pool object size
 - removes the page from _free if all objects allocated

malloc_pool malloc_pools[ilog2_roundup_constexpr(page_size) + 1]

-------------------------------------------------------
- memory::alloc_page() -    1024 < size <= 4096 (page
-------------------------------------------------------
calls memory::untracked_alloc_page()
    ..
    if (!smp_allocator) {
        ret = early_alloc_page();           // calls memory::free_page_ranges.alloc()
    } else {
        ret = page_pool::l1::alloc_page();  // PER_CPU -> calls l1::alloc_page_local()
    }

    per CPU l1 page buffer interact with global_l2 page buffer to fetch/free of pages
    - page_batch* l2::alloc_page_batch() which ends up calling memory::free_page_ranges.alloc() when l2::refill()
    - void l2::free_page_batch(page_batch* pb)
    - --- page_batch has constant of 32 pages of memory
    - --- l2 seems to be keeping at least 8 page batches (8 * 32 * 4K = 1B) per cpu

-------------------------------------------------------
- memory::malloc_large() - size > page
-------------------------------------------------------

******************************************
* 3 ways to free memory (from free) - calls mmu::get_mem_area(object) to read 44-46 bits of an address to determine how memory was allocated
-------------------------------------------------------
- memory::pool::from_object(object)->free(object) -    size <= 1024 (1/4 of page)
-------------------------------------------------------
pool* pool::from_object(void* object)
{
    auto header = to_header(static_cast<free_object*>(object));
    return header->owner;
}

calls pool:free() which checks memory page header on which CPU memory was allocated
- if same cpu it calls
     void pool::free_same_cpu(free_object* obj, unsigned cpu_id) gets page header
       * if page is empty (nalloc == 0) and there is at least another empty page it removes this page from the pool and calls untracked_free_page
       * otherwise it adds freed object in the front of the single-linked list
          obj->next = header->local_free;
          header->local_free = obj;
        - also pushes full page back in the _free list of pages otherwise brings it upfront
- or calls
     void pool::free_different_cpu(free_object* obj, unsigned obj_cpu, unsigned cur_cpu)
       * finds a "sink" for that cpu and calls free on that sink object

-------------------------------------------------------
- memory::free_page() -    1024 < size <= 4096 (page
-------------------------------------------------------

-------------------------------------------------------
- memory::free_large() -   size > page
-------------------------------------------------------
